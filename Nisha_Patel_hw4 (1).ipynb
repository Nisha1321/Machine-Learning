{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrRxexfttGfV"
      },
      "source": [
        "# CS 584 :: Data Mining :: George Mason University :: Fall 2025\n",
        "\n",
        "\n",
        "# Homework 4: Association Rule Mining\n",
        "\n",
        "- **100 points [6% of your final grade]**\n",
        "- **Due Sunday, December 7 by 11:59pm**\n",
        "\n",
        "- *Goals of this homework:* implement the association rule mining process with the Apriori algorithm.\n",
        "\n",
        "- *Submission instructions:* for this homework, you need to submit to Canvas. Please name your submission **FirstName_Lastname_hw4.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw4.ipynb**. Your notebook should be fully executed so that we can see all outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3qB33F3tGfY"
      },
      "source": [
        "In this assignment, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3RgijY4JtGfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a937983e-1da0-412f-a1a7-06dffd3be97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array of user-movie matrix: shape (11743, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "user_movie_data = np.loadtxt(\"/content/drive/MyDrive/HW4_data/movie_rated.txt\", delimiter=',')\n",
        "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBa77Pag3aUr",
        "outputId": "2887c9a5-791b-4e31-ee3b-927b941e5ef4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC93R5K1tGfZ"
      },
      "source": [
        "In this dataset, there are two columns: the first column is the integer IDs of users, and the second column is the integer ids of movies. Each row denotes that the user of the given user id watched the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been watched by a single user as a transaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU-EISRXtGfZ"
      },
      "source": [
        "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user movie-watching behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach.\n",
        "\n",
        "**Note: Do not copy-paste any existing code.**\n",
        "\n",
        "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
        "\n",
        "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will explain the process of the method.**\n",
        "\n",
        "**Hint: If you implement your algorithm correctly, you should be able to see the intermediate result as:**\n",
        "- Candidates of length 1 count: 408\n",
        "- After Pruning count: 21\n",
        "- Candidates of length 2 count: 210\n",
        "- After Pruning 2 count: 36\n",
        "- Candidates of length 3 count: 55\n",
        "- After Pruning 3 count: 12\n",
        "- Candidates of length 4 count: 1\n",
        "- After Pruning 4 count: 0\n",
        "\n",
        "**Hint: \"Candidates of length 1/2/3/4 count\" can be different, depending on what methods you use to generate candidate sets. But, your \"After Pruning count\" should be the same as what is shown above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pmOOB5UntGfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46abb1b9-37bb-4fa4-ebb0-939dc566ba1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total transactions (users): 494\n",
            "Unique items (movies): 408\n",
            "Sample transactions (up to 3): [frozenset({2160, 2312, 144, 480}), frozenset({480, 2160, 1221, 2890, 1228}), frozenset({1270})]\n",
            "=== Running Apriori ===\n",
            "Candidates of length 1 count: 408\n",
            "After Pruning 1 count: 21\n",
            "Candidates of length 2 count: 210\n",
            "After Pruning 2 count: 36\n",
            "Candidates of length 3 count: 123\n",
            "After Pruning 3 count: 12\n",
            "Candidates of length 4 count: 15\n",
            "After Pruning 4 count: 0\n",
            "=== Frequent itemsets summary (k, count) ===\n",
            "k=1: 21 frequent itemsets\n",
            "k=2: 36 frequent itemsets\n",
            "k=3: 12 frequent itemsets\n",
            "Total rules stored (not printed here): 14\n"
          ]
        }
      ],
      "source": [
        "# Write your code\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "# 1) Build transactions: one user = one transaction\n",
        "assert user_movie_data.ndim == 2 and user_movie_data.shape[1] == 2, \"Expected 2 columns: userId,movieId\"\n",
        "\n",
        "transactions_by_user = defaultdict(set)\n",
        "for uid, mid in user_movie_data:\n",
        "    transactions_by_user[int(uid)].add(int(mid))\n",
        "\n",
        "transactions = [frozenset(mset) for mset in transactions_by_user.values() if mset]\n",
        "N = len(transactions)\n",
        "universe_items = sorted({m for t in transactions for m in t})\n",
        "\n",
        "print(f\"Total transactions (users): {N}\")\n",
        "print(f\"Unique items (movies): {len(universe_items)}\")\n",
        "print(\"Sample transactions (up to 3):\", list(transactions)[:3])\n",
        "\n",
        "# 2) Helpers for Apriori\n",
        "def compute_support_counts(transactions, candidates):\n",
        "    \"\"\"Scan once to count how many transactions contain each candidate itemset.\"\"\"\n",
        "    counts = defaultdict(int)\n",
        "    for t in transactions:\n",
        "        for c in candidates:\n",
        "            if c.issubset(t):\n",
        "                counts[c] += 1\n",
        "    return counts\n",
        "\n",
        "def generate_candidates(prev_frequents, k):\n",
        "    \"\"\"\n",
        "    Self-join step: generate k-itemset candidates from frequent (k-1)-itemsets.\n",
        "    We use simple set union and keep only those whose size becomes exactly k.\n",
        "    \"\"\"\n",
        "    prev = sorted(list(prev_frequents))\n",
        "    Ck = set()\n",
        "    for i in range(len(prev)):\n",
        "        for j in range(i+1, len(prev)):\n",
        "            u = prev[i].union(prev[j])\n",
        "            if len(u) == k:\n",
        "                Ck.add(frozenset(u))\n",
        "    return Ck\n",
        "\n",
        "def prune_with_apriori_property(candidates, prev_frequents, k):\n",
        "    \"\"\"\n",
        "    candidates are size-k itemsets.\n",
        "    Keep candidate c only if ALL (k-1)-subsets of c are in prev_frequents (size k-1 frequents).\n",
        "    \"\"\"\n",
        "    prev_frequents = set(prev_frequents)\n",
        "    pruned = set()\n",
        "    for c in candidates:\n",
        "        ok = True\n",
        "        for sub in combinations(c, k-1):   # <-- (k-1)-subsets\n",
        "            if frozenset(sub) not in prev_frequents:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            pruned.add(c)\n",
        "    return pruned\n",
        "\n",
        "\n",
        "def filter_by_support(transactions, candidates, min_support, label=\"\"):\n",
        "    \"\"\"\n",
        "    Keep only candidates whose support >= min_support.\n",
        "    Prints \"After Pruning {label} count: X\" per the assignment instructions.\n",
        "    Returns: (frequents_set, support_map_for_these, raw_counts)\n",
        "    \"\"\"\n",
        "    counts = compute_support_counts(transactions, candidates)\n",
        "    frequents, sup_map = set(), {}\n",
        "    N = len(transactions)\n",
        "    for itemset, cnt in counts.items():\n",
        "        sup = cnt / N\n",
        "        if sup >= min_support:\n",
        "            frequents.add(itemset)\n",
        "            sup_map[itemset] = sup\n",
        "    if label:\n",
        "        print(f\"After Pruning {label} count: {len(frequents)}\")\n",
        "    return frequents, sup_map, counts\n",
        "\n",
        "def apriori(transactions, min_support=0.2, verbose=True):\n",
        "    \"\"\"\n",
        "    Mine all frequent itemsets using Apriori.\n",
        "    Returns:\n",
        "      all_frequents: dict size k -> set of frequent k-itemsets\n",
        "      support_map : dict itemset -> support (fraction)\n",
        "    \"\"\"\n",
        "    # 1-item candidates\n",
        "    C1 = set(frozenset([i]) for i in sorted({m for t in transactions for m in t}))\n",
        "    if verbose:\n",
        "        print(f\"Candidates of length 1 count: {len(C1)}\")\n",
        "    L1, sup1, _ = filter_by_support(transactions, C1, min_support, label=\"1\")\n",
        "\n",
        "    all_frequents = {1: L1}\n",
        "    support_map = dict(sup1)\n",
        "\n",
        "    k = 2\n",
        "    L_prev = L1\n",
        "    while L_prev:\n",
        "        # Generate apriori-prune candidates of size k\n",
        "        Ck = generate_candidates(L_prev, k)\n",
        "        if verbose:\n",
        "            print(f\"Candidates of length {k} count: {len(Ck)}\")\n",
        "\n",
        "        if k >= 3:\n",
        "            Ck = prune_with_apriori_property(Ck, L_prev, k)\n",
        "\n",
        "\n",
        "        Lk, supk, _ = filter_by_support(transactions, Ck, min_support, label=str(k))\n",
        "        if not Lk:\n",
        "            break\n",
        "\n",
        "        all_frequents[k] = Lk\n",
        "        support_map.update(supk)\n",
        "        L_prev = Lk\n",
        "        k += 1\n",
        "\n",
        "    return all_frequents, support_map\n",
        "\n",
        "def all_nonempty_proper_subsets(itemset):\n",
        "    \"\"\"Yield all non-empty proper subsets of itemset.\"\"\"\n",
        "    s = list(itemset)\n",
        "    for r in range(1, len(s)):\n",
        "        for comb in combinations(s, r):\n",
        "            yield frozenset(comb)\n",
        "\n",
        "def generate_rules(all_frequents, support_map, min_confidence=0.8):\n",
        "    \"\"\"\n",
        "    Generate rules A -> B where confidence >= min_confidence.\n",
        "    Returns a list of dicts (antecedent, consequent, support, confidence, lift).\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    # consider only frequent itemsets of size >= 2\n",
        "    for k, sets_k in all_frequents.items():\n",
        "        if k < 2:\n",
        "            continue\n",
        "        for I in sets_k:\n",
        "            s_I = support_map[I]\n",
        "            for A in all_nonempty_proper_subsets(I):\n",
        "                B = I - A\n",
        "                if not B:\n",
        "                    continue\n",
        "                s_A = support_map.get(A, 0.0)\n",
        "                if s_A == 0:\n",
        "                    continue\n",
        "                conf = s_I / s_A\n",
        "                if conf >= min_confidence:\n",
        "                    s_B = support_map.get(B, 0.0)\n",
        "                    lift = (s_I / (s_A * s_B)) if s_B > 0 else float(\"nan\")\n",
        "                    rules.append({\n",
        "                        \"antecedent\": A,\n",
        "                        \"consequent\": B,\n",
        "                        \"support\": s_I,\n",
        "                        \"confidence\": conf,\n",
        "                        \"lift\": lift\n",
        "                    })\n",
        "    # sort for consistent viewing (confidence desc, support desc, then shorter rules first)\n",
        "    rules.sort(key=lambda r: (-r[\"confidence\"], -r[\"support\"], len(r[\"antecedent\"]) + len(r[\"consequent\"])))\n",
        "    return rules\n",
        "\n",
        "#  3) Run Apriori with required thresholds & print intermediate steps\n",
        "MIN_SUPPORT = 0.2\n",
        "MIN_CONFIDENCE = 0.8\n",
        "\n",
        "print(\"=== Running Apriori ===\")\n",
        "all_frequents, support_map = apriori(transactions, min_support=MIN_SUPPORT, verbose=True)\n",
        "\n",
        "print(\"=== Frequent itemsets summary (k, count) ===\")\n",
        "for k in sorted(all_frequents.keys()):\n",
        "    print(f\"k={k}: {len(all_frequents[k])} frequent itemsets\")\n",
        "\n",
        "rules_found = generate_rules(all_frequents, support_map, min_confidence=MIN_CONFIDENCE)\n",
        "print(f\"Total rules stored (not printed here): {len(rules_found)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQdE-kKtGfa"
      },
      "source": [
        "Finally, print your final association rules in the following format:\n",
        "\n",
        "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
        "\n",
        "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
        "\n",
        "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
        "\n",
        "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as using pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**\n",
        "\n",
        "**Hint: if you implement the algorithm correctly, you will find 14 rules in total:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vb-RaBqrtGfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034129b6-e144-4db7-ec3c-e5e4bbfa8a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Final Association Rules ===\n",
            "Star Wars: Episode IV - A New Hope (1977), Godfather: Part II, The (1974) --> Godfather, The (1972)\n",
            "Jurassic Park (1993), Princess Bride, The (1987) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Schindler's List (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Princess Bride, The (1987), Saving Private Ryan (1998) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Godfather, The (1972), Godfather: Part II, The (1974) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Princess Bride, The (1987), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Godfather: Part II, The (1974) --> Godfather, The (1972)\n",
            "Back to the Future (1985), Saving Private Ryan (1998) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Star Wars: Episode IV - A New Hope (1977), Groundhog Day (1993) --> Back to the Future (1985)\n",
            "Jurassic Park (1993), Saving Private Ryan (1998) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Princess Bride, The (1987), Groundhog Day (1993) --> Back to the Future (1985)\n",
            "Jaws (1975) --> Star Wars: Episode IV - A New Hope (1977)\n",
            "Star Wars: Episode IV - A New Hope (1977), Princess Bride, The (1987) --> Back to the Future (1985)\n",
            "\n",
            " Matches the hint: 14 rules found.\n"
          ]
        }
      ],
      "source": [
        "# Write your code to print out the rules\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load movies.csv and build a lookup: movieId -> title\n",
        "\n",
        "movies_df = pd.read_csv('/content/drive/MyDrive/HW4_data/movies.csv')\n",
        "\n",
        "df = pd.DataFrame(movies_df)\n",
        "df.head()\n",
        "#  file uses: movieId, movie_name\n",
        "assert {'movieId', 'movie_name'}.issubset(df.columns), \\\n",
        "    \"movies.csv must have columns: movieId, movie_name\"\n",
        "\n",
        "# Build lookup dict {movieId -> movie_name}\n",
        "id_to_title = dict(zip(df['movieId'].astype(int), df['movie_name'].astype(str)))\n",
        "\n",
        "def ids_to_titles(id_iterable):\n",
        "    ids_sorted = sorted(int(x) for x in id_iterable)\n",
        "    return [id_to_title.get(i, f\"<movieId {i}>\") for i in ids_sorted]\n",
        "\n",
        "def print_rule_human(rule):\n",
        "    left  = ', '.join(ids_to_titles(rule['antecedent']))\n",
        "    right = ', '.join(ids_to_titles(rule['consequent']))\n",
        "    sup = rule.get('support', float('nan'))\n",
        "    conf = rule.get('confidence', float('nan'))\n",
        "    lift = rule.get('lift', float('nan'))\n",
        "    print(f\"{left} --> {right}  (support={sup:.3f}, confidence={conf:.3f}, lift={lift:.3f})\")\n",
        "\n",
        "# Ensure rules are present\n",
        "try:\n",
        "    _ = rules_found\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\"`rules_found` is not defined. Run the Apriori cell first.\") from e\n",
        "\n",
        "print(\"=== Final Association Rules ===\")\n",
        "for r in rules_found:\n",
        "    left = ', '.join(ids_to_titles(r[\"antecedent\"]))\n",
        "    right = ', '.join(ids_to_titles(r[\"consequent\"]))\n",
        "    print(f\"{left} --> {right}\")\n",
        "\n",
        "\n",
        "\n",
        "if len(rules_found) == 14:\n",
        "    print(\"\\n Matches the hint: 14 rules found.\")\n",
        "else:\n",
        "    print(f\"\\n Hint says 14 rules; {len(rules_found)} (can differ if data/thresholds differ).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygH5mRm6hQ46"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}