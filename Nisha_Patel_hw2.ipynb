{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9bUgPoIJ9t7"
      },
      "source": [
        "# CS 584 :: Data Mining :: George Mason University :: Fall 2025\n",
        "\n",
        "\n",
        "# Homework 2: Linear Regression&Neural Networks\n",
        "\n",
        "- **100 points [6% of your final grade]**\n",
        "- **Due Sunday, October 19 by 11:59pm**\n",
        "\n",
        "- *Goals of this homework:* (1) implement the linear regression model; (2) implement the multi-layer perceptron neural network; (3) tune the hyperparameters of MLP model to produce classification result as good as possible.\n",
        "\n",
        "- *Submission instructions:* for this homework, you should submit your notebook file to **Canvas** (look for the homework 2 assignment there). Please name your submission **FirstName_Lastname_hw2.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw2.ipynb**. Your notebook should be fully executed so that we can see all outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDcOcCNJ9t8"
      },
      "source": [
        "## Part 1: Linear Regression (40 points)\n",
        "\n",
        "Recent studies have found that novel mobile games can lead to increased physical activity. A notable example is Pokemon Go, a mobile game combining the Pokemon world through augmented reality with the real world requiring players to physically move around. Specifically, in the following study, researchers have found that Pokemon Go leads to increased levels of physical activity for the most engaged players! https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5174727/\n",
        "![image.png](attachment:image.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkvv5n6BNltu",
        "outputId": "b38d95b9-fc47-4f60-b640-a1d99136e6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wZknh_BJ9t9"
      },
      "source": [
        "In this part, our goal is to predict the combat point of each pokemon in the 2017 Pokemon Go mobile game. Each pokemon has its own unique attributes that can help predicting its combat points. These include:\n",
        "\n",
        "- Stamina\n",
        "- Attack value\n",
        "- Defense value\n",
        "- Capture rate\n",
        "- Flee rate\n",
        "- Spawn chance\n",
        "- Primary strength\n",
        "\n",
        "The file pokemon_data.csv contains data of 146 pokemons to be used in this homework. The rows of these files refer to the data samples (i.e., pokemon samples), while the columns denote the name of the pokemon (column 1), its attributes (columns 2-8), and the combat point outcome (column 9). You can ignore column 1 for the rest of this problem.\n",
        "\n",
        "First, let's load the data by executing the following code.\n",
        "\n",
        "**Note: you need to install the pandas library beforehand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "L1ZdlOM3J9t9",
        "outputId": "da6cad0e-1ec5-4730-8702-9740a0d86583"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         name  stamina  attack_value  defense_value  capture_rate  flee_rate  \\\n",
              "0   Bulbasaur       90           126            126          0.16       0.10   \n",
              "1     Ivysaur      120           156            158          0.08       0.07   \n",
              "2    Venusaur      160           198            200          0.04       0.05   \n",
              "3  Charmander       78           128            108          0.16       0.10   \n",
              "4  Charmeleon      116           160            140          0.08       0.07   \n",
              "\n",
              "   spawn_chance primary_strength  combat_point  \n",
              "0          69.0            Grass          1079  \n",
              "1           4.2            Grass          1643  \n",
              "2           1.7            Grass          2598  \n",
              "3          25.3             Fire           962  \n",
              "4           1.2             Fire          1568  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0574895e-a47c-44cb-83ba-f0818fc4dd17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>stamina</th>\n",
              "      <th>attack_value</th>\n",
              "      <th>defense_value</th>\n",
              "      <th>capture_rate</th>\n",
              "      <th>flee_rate</th>\n",
              "      <th>spawn_chance</th>\n",
              "      <th>primary_strength</th>\n",
              "      <th>combat_point</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bulbasaur</td>\n",
              "      <td>90</td>\n",
              "      <td>126</td>\n",
              "      <td>126</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>69.0</td>\n",
              "      <td>Grass</td>\n",
              "      <td>1079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ivysaur</td>\n",
              "      <td>120</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Grass</td>\n",
              "      <td>1643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Venusaur</td>\n",
              "      <td>160</td>\n",
              "      <td>198</td>\n",
              "      <td>200</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.7</td>\n",
              "      <td>Grass</td>\n",
              "      <td>2598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Charmander</td>\n",
              "      <td>78</td>\n",
              "      <td>128</td>\n",
              "      <td>108</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>25.3</td>\n",
              "      <td>Fire</td>\n",
              "      <td>962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Charmeleon</td>\n",
              "      <td>116</td>\n",
              "      <td>160</td>\n",
              "      <td>140</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.2</td>\n",
              "      <td>Fire</td>\n",
              "      <td>1568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0574895e-a47c-44cb-83ba-f0818fc4dd17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0574895e-a47c-44cb-83ba-f0818fc4dd17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0574895e-a47c-44cb-83ba-f0818fc4dd17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-eab529ca-b2d4-4963-ac8e-80d2b17e9617\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eab529ca-b2d4-4963-ac8e-80d2b17e9617')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-eab529ca-b2d4-4963-ac8e-80d2b17e9617 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_frame",
              "summary": "{\n  \"name\": \"data_frame\",\n  \"rows\": 146,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 145,\n        \"samples\": [\n          \"Victreebel\",\n          \"Aerodactyl\",\n          \"Sandslash\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stamina\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 20,\n        \"max\": 500,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          180,\n          70,\n          110\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attack_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 40,\n        \"max\": 250,\n        \"num_unique_values\": 67,\n        \"samples\": [\n          134,\n          102,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"defense_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37,\n        \"min\": 54,\n        \"max\": 222,\n        \"num_unique_values\": 65,\n        \"samples\": [\n          168,\n          134,\n          126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capture_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12254951858892366,\n        \"min\": 0.04,\n        \"max\": 0.56,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.1,\n          0.16,\n          0.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flee_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0824460717516844,\n        \"min\": 0.05,\n        \"max\": 0.99,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07,\n          0.06,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spawn_chance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 198.5012730720114,\n        \"min\": 0.01,\n        \"max\": 1598.0,\n        \"num_unique_values\": 104,\n        \"samples\": [\n          131.0,\n          3.6,\n          105.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_strength\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Fighting\",\n          \"Rock\",\n          \"Grass\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"combat_point\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 689,\n        \"min\": 264,\n        \"max\": 3525,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          2548,\n          2180,\n          1823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_frame = pd.read_csv('/content/drive/MyDrive/hw2/pokemon_data.csv')\n",
        "data_frame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1QPoaE9J9t-"
      },
      "source": [
        "Pandas is a fast, powerful, flexible and easy-to-use open-source data analysis and manipulation tool, built on top of Python. By executing the following code, let's create one Numpy array to contain the feature data without the name column and one array to contain the combat point ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EehstFWJ9t-",
        "outputId": "d9870d73-e20b-41ad-c31b-40eb82d8cc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array of labels: shape (146,)\n",
            "array of feature matrix: shape (146, 7)\n"
          ]
        }
      ],
      "source": [
        "features = data_frame.values[:, 1:-1]\n",
        "labels = data_frame.values[:, -1]\n",
        "print('array of labels: shape ' + str(np.shape(labels)))\n",
        "print('array of feature matrix: shape ' + str(np.shape(features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsatVtdOJ9t_"
      },
      "source": [
        "Now, you may find out that we have a categorical feature 'primary_strength' in our data. Categorical features require special attention because usually they cannot be the input of regression models as they are. A potential way to treat categorical features is to simply convert each value of the feature to a separate number. However, this might impute non-existent relative associations between the features, which might not always be representative of the data (e.g., if we assign ‚Äú1‚Äù to the value ‚Äúgreen‚Äù and ‚Äú2‚Äù to the value ‚Äúred‚Äù, the regression algorithm will assume that ‚Äúred‚Äù is greater than ‚Äúgreen,‚Äù which is not necessarily the case). For this reason, we can use a ‚Äúone hot encoding‚Äù to represent categorical features. According to this, we will create a binary column for each category of the categorical feature, which will take a value of 1 if the sample belongs to that category, and 0 otherwise. For each categorical feature of the problem, count the number of different values and implement the one hot encoding. For the remaining of the problem, you will be working with the one hot encoding of the categorical features.\n",
        "\n",
        "\n",
        "In the next cell, write your code to replace the categorical feature 'primary_strength' with **one-hot encoding** and generate the new version of the Numpy array 'features'.\n",
        "\n",
        "**Hint: if you don't remember one hot encoding, review the slides of our first-week lecture.**\n",
        "\n",
        "**Note: do not use sklearn to automatically generate one hot encoding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRCPTrbLJ9t_",
        "outputId": "b06d3d0d-96b7-4aee-b82e-b47abb58fcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'primary_strength': ['Bug' 'Dragon' 'Electric' 'Fairy' 'Fighting' 'Fire' 'Ghost' 'Grass'\n",
            " 'Ground' 'Ice' 'Normal' 'Poison' 'Psychic' 'Rock' 'Water']\n",
            "New features shape after encoding: (146, 21)\n"
          ]
        }
      ],
      "source": [
        "# --- One-Hot Encoding for 'primary_strength' (no sklearn) ---\n",
        "\n",
        "# extract the categorical column\n",
        "primary_strength = features[:, -1]  # last column\n",
        "\n",
        "# find unique categories\n",
        "unique_strengths = np.unique(primary_strength)\n",
        "print(\"Unique values in 'primary_strength':\", unique_strengths)\n",
        "\n",
        "# create an empty one-hot matrix\n",
        "one_hot = np.zeros((len(primary_strength), len(unique_strengths)))\n",
        "\n",
        "# fill one-hot matrix\n",
        "for i, val in enumerate(primary_strength):\n",
        "    idx = np.where(unique_strengths == val)[0][0]\n",
        "    one_hot[i, idx] = 1\n",
        "\n",
        "# remove the original categorical column\n",
        "features_numeric = features[:, :-1]\n",
        "\n",
        "# combine numeric and one-hot encoded columns\n",
        "features = np.hstack((features_numeric, one_hot))\n",
        "\n",
        "\n",
        "print(\"New features shape after encoding:\", features.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voaTcpTfJ9t_"
      },
      "source": [
        "Besides, you may also notice that other features have different scales. So, you need to standardize them: $({x-\\mu})/{\\sigma}$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation. Write your code below.\n",
        "\n",
        "**Hint: details about feature standardization is also in slides of our first-week lecture.**\n",
        "\n",
        "**Note: You do not need to do standardize for one-hot encodings.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgLNV_5RJ9t_",
        "outputId": "4af33686-da85-445f-c1e8-1a6770902fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of numeric columns:\n",
            " [1.26301370e+02 1.48273973e+02 1.44178082e+02 2.30958904e-01\n",
            " 9.45205479e-02 7.57054110e+01]\n",
            "\n",
            "Standard deviation of numeric columns:\n",
            " [5.67191314e+01 4.02646248e+01 3.77648973e+01 1.22129107e-01\n",
            " 8.21632371e-02 1.97820306e+02]\n",
            "\n",
            "New shape of standardized features: (146, 21)\n",
            "\n",
            "=== Columns after standardization ===\n",
            "['stamina', 'attack', 'defense', 'capture_rate', 'flee_rate', 'spawn_chance', 'primary_strength_Bug', 'primary_strength_Dragon', 'primary_strength_Electric', 'primary_strength_Fairy', 'primary_strength_Fighting', 'primary_strength_Fire', 'primary_strength_Ghost', 'primary_strength_Grass', 'primary_strength_Ground', 'primary_strength_Ice', 'primary_strength_Normal', 'primary_strength_Poison', 'primary_strength_Psychic', 'primary_strength_Rock', 'primary_strength_Water']\n",
            "\n",
            "=== First 5 rows (after standardization) ===\n",
            "    stamina    attack   defense capture_rate flee_rate spawn_chance  \\\n",
            "0  -0.64002  -0.55319 -0.481349    -0.581015   0.06669    -0.033896   \n",
            "1 -0.111098  0.191881  0.365999     -1.23606 -0.298437    -0.361466   \n",
            "2  0.594132  1.234981  1.478143    -1.563582 -0.541855    -0.374104   \n",
            "3 -0.851589 -0.503518 -0.957982    -0.581015   0.06669    -0.254804   \n",
            "4 -0.181621  0.291224 -0.110634     -1.23606 -0.298437    -0.376632   \n",
            "\n",
            "  primary_strength_Bug primary_strength_Dragon primary_strength_Electric  \\\n",
            "0                  0.0                     0.0                       0.0   \n",
            "1                  0.0                     0.0                       0.0   \n",
            "2                  0.0                     0.0                       0.0   \n",
            "3                  0.0                     0.0                       0.0   \n",
            "4                  0.0                     0.0                       0.0   \n",
            "\n",
            "  primary_strength_Fairy  ... primary_strength_Fire primary_strength_Ghost  \\\n",
            "0                    0.0  ...                   0.0                    0.0   \n",
            "1                    0.0  ...                   0.0                    0.0   \n",
            "2                    0.0  ...                   0.0                    0.0   \n",
            "3                    0.0  ...                   1.0                    0.0   \n",
            "4                    0.0  ...                   1.0                    0.0   \n",
            "\n",
            "  primary_strength_Grass primary_strength_Ground primary_strength_Ice  \\\n",
            "0                    1.0                     0.0                  0.0   \n",
            "1                    1.0                     0.0                  0.0   \n",
            "2                    1.0                     0.0                  0.0   \n",
            "3                    0.0                     0.0                  0.0   \n",
            "4                    0.0                     0.0                  0.0   \n",
            "\n",
            "  primary_strength_Normal primary_strength_Poison primary_strength_Psychic  \\\n",
            "0                     0.0                     0.0                      0.0   \n",
            "1                     0.0                     0.0                      0.0   \n",
            "2                     0.0                     0.0                      0.0   \n",
            "3                     0.0                     0.0                      0.0   \n",
            "4                     0.0                     0.0                      0.0   \n",
            "\n",
            "  primary_strength_Rock primary_strength_Water  \n",
            "0                   0.0                    0.0  \n",
            "1                   0.0                    0.0  \n",
            "2                   0.0                    0.0  \n",
            "3                   0.0                    0.0  \n",
            "4                   0.0                    0.0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# --- STANDARDIZATION OF NUMERIC FEATURES ONLY ---\n",
        "''' When we ‚Äústandardize‚Äù data, we make all numbers fair to compare.\n",
        "Some features (like attack power) might have big numbers, while others (like flee rate) are tiny decimals.\n",
        "If we don‚Äôt fix this, the big numbers can dominate the model and confuse it.\n",
        "It‚Äôs like making all features ‚Äúspeak the same language‚Äù before giving them to the model.\n",
        "\n",
        "So instead of:\n",
        "\n",
        "‚ÄúAttack is in hundreds, spawn chance is in decimals, stamina is in tens‚Ä¶‚Äù\n",
        "\n",
        "After standardization:\n",
        "\n",
        "‚ÄúEverything is now on the same 0‚Äì1 or ‚Äì1 to +1 scale.‚Äù '''\n",
        "\n",
        "# --- STANDARDIZATION OF NUMERIC FEATURES ONLY ---\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# how many numeric columns we have before one-hot columns\n",
        "# (in pokemon_data.csv, first 6 columns are numeric)\n",
        "num_numeric = 6\n",
        "\n",
        "# separate numeric and one-hot parts\n",
        "numeric_part = features[:, :num_numeric].astype(float)\n",
        "one_hot_part = features[:, num_numeric:]\n",
        "\n",
        "# calculate mean (Œº) and standard deviation (œÉ) for each numeric column\n",
        "mean = np.mean(numeric_part, axis=0)\n",
        "std = np.std(numeric_part, axis=0)\n",
        "\n",
        "# to avoid division by zero, replace any zero std with 1\n",
        "std[std == 0] = 1\n",
        "\n",
        "# apply standardization formula: (x - Œº) / œÉ\n",
        "numeric_standardized = (numeric_part - mean) / std\n",
        "\n",
        "# combine back standardized numeric + original one-hot columns\n",
        "features = np.hstack((numeric_standardized, one_hot_part))\n",
        "\n",
        "# print mean and std info\n",
        "print(\"Mean of numeric columns:\\n\", mean)\n",
        "print(\"\\nStandard deviation of numeric columns:\\n\", std)\n",
        "print(\"\\nNew shape of standardized features:\", features.shape)\n",
        "\n",
        "# --- PRINT COLUMN NAMES AND FIRST 5 ROWS AFTER STANDARDIZATION ---\n",
        "\n",
        "# define the first 6 numeric column names (based on pokemon_data.csv)\n",
        "numeric_cols = [\"stamina\", \"attack\", \"defense\", \"capture_rate\", \"flee_rate\", \"spawn_chance\"]\n",
        "\n",
        "# for the one-hot columns, use the category names from before\n",
        "# (make sure you ran the one-hot encoding cell that defined `unique_strengths`)\n",
        "one_hot_cols = [f\"primary_strength_{str(s)}\" for s in unique_strengths]\n",
        "\n",
        "# combine all column names\n",
        "all_columns = numeric_cols + one_hot_cols\n",
        "\n",
        "# create a DataFrame to preview the features\n",
        "features_df = pd.DataFrame(features, columns=all_columns)\n",
        "\n",
        "print(\"\\n=== Columns after standardization ===\")\n",
        "print(features_df.columns.tolist())\n",
        "\n",
        "print(\"\\n=== First 5 rows (after standardization) ===\")\n",
        "print(features_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f66GRvMJ9uA"
      },
      "source": [
        "Now, in the next cell, you need to implement your own **linear regression model** using the **Ordinary Least Squares (OLS)** solution **without regularization**. And here, you should adopt the **5-fold cross-validation** method. For each fold compute and print out the **square root** of the residual sum of squares error (RSS) between the actual and predicted outcome variable. Also compute and print out the **average** square root of the RSS over all folds.\n",
        "\n",
        "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn.**\n",
        "\n",
        "**Hint: Use numpy.linalg.pinv() for calculating the inverse of a matrix.**\n",
        "\n",
        "**Hint: details about cross-validation is in slides of KNN lecture.**\n",
        "\n",
        "**Hint: the sqrt_RSS for each fold should be 0~3000**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "istkXFU2J9uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e7abbb-14f7-47c5-bfdb-a0b34edd5b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: sqrt_RSS = 621.6119\n",
            "Fold 2: sqrt_RSS = 772.9777\n",
            "Fold 3: sqrt_RSS = 555.7166\n",
            "Fold 4: sqrt_RSS = 1377.2311\n",
            "Fold 5: sqrt_RSS = 1530.0825\n",
            "\n",
            "Average sqrt_RSS over 5 folds = 971.5239\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# --- OLS LINEAR REGRESSION WITH 5-FOLD CROSS-VALIDATION (no sklearn) ---\n",
        "# Assumes:\n",
        "#   - `features` is your final feature matrix (one-hot done; numeric optionally standardized)\n",
        "#   - `labels` is your target vector\n",
        "# Outputs:\n",
        "#   - sqrt_RSS per fold and the average sqrt_RSS\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = features.astype(float)\n",
        "y = labels.astype(float).ravel()  # ensure 1D\n",
        "\n",
        "# Add bias (intercept) column helper\n",
        "def add_bias(A):\n",
        "    return np.hstack([np.ones((A.shape[0], 1)), A])\n",
        "\n",
        "# Make 5 folds (shuffled, reproducible)\n",
        "rng = np.random.default_rng(42)\n",
        "idx = np.arange(X.shape[0])\n",
        "rng.shuffle(idx)\n",
        "folds = np.array_split(idx, 5)\n",
        "\n",
        "sqrt_rss_per_fold = []\n",
        "\n",
        "for k in range(5):\n",
        "    val_idx = folds[k]\n",
        "    train_idx = np.concatenate([folds[i] for i in range(5) if i != k])\n",
        "\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val,   y_val   = X[val_idx],   y[val_idx]\n",
        "\n",
        "    # Add bias\n",
        "    X_train_b = add_bias(X_train)\n",
        "    X_val_b   = add_bias(X_val)\n",
        "\n",
        "    # OLS with pseudo-inverse: theta = pinv(X) y  (equivalently pinv(X^T X) X^T y)\n",
        "    theta = np.linalg.pinv(X_train_b) @ y_train\n",
        "\n",
        "    # Predict on validation\n",
        "    y_hat = X_val_b @ theta\n",
        "\n",
        "    # RSS and sqrt(RSS)\n",
        "    rss = np.sum((y_val - y_hat) ** 2)\n",
        "    sqrt_rss = np.sqrt(rss)\n",
        "    sqrt_rss_per_fold.append(sqrt_rss)\n",
        "\n",
        "    print(f\"Fold {k+1}: sqrt_RSS = {sqrt_rss:.4f}\")\n",
        "\n",
        "avg_sqrt_rss = np.mean(sqrt_rss_per_fold)\n",
        "print(f\"\\nAverage sqrt_RSS over 5 folds = {avg_sqrt_rss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSvSOQ05J9uA"
      },
      "source": [
        "At the end in this part, please repeat the same experiment as in the previous step, but instead of linear regression, implement linear regression **with L2-norm regularization**. Experiment and report your results (average square root of RSS over 5-fold cross-validation) with different values of the regularization term $\\lambda=\\{1, 0.1, 0.01, 0.001, 0.0001\\}$.\n",
        "\n",
        "**Hint: details about the closed-form solution with regularization is on page 74 in slides of our linear regression lecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ_QyPJeJ9uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3b81d6-3263-43eb-c9e0-4f5cc385db50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Œª = 1 ===\n",
            "Fold 1: sqrt_RSS = 596.1611\n",
            "Fold 2: sqrt_RSS = 766.0234\n",
            "Fold 3: sqrt_RSS = 526.5059\n",
            "Fold 4: sqrt_RSS = 1140.9784\n",
            "Fold 5: sqrt_RSS = 1527.3252\n",
            "Average sqrt_RSS over 5 folds (Œª=1) = 911.3988\n",
            "\n",
            "=== Œª = 0.1 ===\n",
            "Fold 1: sqrt_RSS = 617.9304\n",
            "Fold 2: sqrt_RSS = 772.1057\n",
            "Fold 3: sqrt_RSS = 551.7590\n",
            "Fold 4: sqrt_RSS = 1343.3902\n",
            "Fold 5: sqrt_RSS = 1529.5176\n",
            "Average sqrt_RSS over 5 folds (Œª=0.1) = 962.9406\n",
            "\n",
            "=== Œª = 0.01 ===\n",
            "Fold 1: sqrt_RSS = 621.2279\n",
            "Fold 2: sqrt_RSS = 772.8880\n",
            "Fold 3: sqrt_RSS = 555.3049\n",
            "Fold 4: sqrt_RSS = 1374.1429\n",
            "Fold 5: sqrt_RSS = 1529.8394\n",
            "Average sqrt_RSS over 5 folds (Œª=0.01) = 970.6806\n",
            "\n",
            "=== Œª = 0.001 ===\n",
            "Fold 1: sqrt_RSS = 621.5733\n",
            "Fold 2: sqrt_RSS = 772.9687\n",
            "Fold 3: sqrt_RSS = 555.6753\n",
            "Fold 4: sqrt_RSS = 1377.3758\n",
            "Fold 5: sqrt_RSS = 1529.8729\n",
            "Average sqrt_RSS over 5 folds (Œª=0.001) = 971.4932\n",
            "\n",
            "=== Œª = 0.0001 ===\n",
            "Fold 1: sqrt_RSS = 621.6080\n",
            "Fold 2: sqrt_RSS = 772.9768\n",
            "Fold 3: sqrt_RSS = 555.7125\n",
            "Fold 4: sqrt_RSS = 1377.7007\n",
            "Fold 5: sqrt_RSS = 1529.8763\n",
            "Average sqrt_RSS over 5 folds (Œª=0.0001) = 971.5748\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# --- RIDGE (L2) LINEAR REGRESSION WITH 5-FOLD CROSS-VALIDATION (no sklearn) ---\n",
        "# Assumes:\n",
        "#   - `features` is your final feature matrix (one-hot done; numeric optionally standardized)\n",
        "#   - `labels` is your target vector\n",
        "# What this does:\n",
        "#   - Repeats the OLS experiment but with L2 regularization (Ridge)\n",
        "#   - Tests multiple lambda values and reports sqrt_RSS for each fold + average\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = features.astype(float)\n",
        "y = labels.astype(float).ravel()  # ensure 1D\n",
        "\n",
        "# Helper: add a bias (intercept) column of ones\n",
        "def add_bias(A):\n",
        "    return np.hstack([np.ones((A.shape[0], 1)), A])\n",
        "\n",
        "# Ridge closed-form solution:\n",
        "# theta = (X^T X + Œª * R)^(-1) X^T y\n",
        "# where R is identity *except* we do NOT regularize the bias term (R[0,0] = 0)\n",
        "def ridge_fit(Xb, y, lam):\n",
        "    d = Xb.shape[1]\n",
        "    R = np.eye(d)\n",
        "    R[0, 0] = 0.0  # do not regularize bias\n",
        "    XtX = Xb.T @ Xb\n",
        "    Xty = Xb.T @ y\n",
        "    theta = np.linalg.pinv(XtX + lam * R) @ Xty\n",
        "    return theta\n",
        "\n",
        "def sqrt_rss(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred) ** 2))\n",
        "\n",
        "# Make 5 folds (shuffled, reproducible)\n",
        "rng = np.random.default_rng(42)\n",
        "idx = np.arange(X.shape[0])\n",
        "rng.shuffle(idx)\n",
        "folds = np.array_split(idx, 5)\n",
        "\n",
        "# Add bias once\n",
        "Xb = add_bias(X)\n",
        "\n",
        "lambdas = [1, 0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "for lam in lambdas:\n",
        "    sqrt_rss_per_fold = []\n",
        "    print(f\"\\n=== Œª = {lam} ===\")\n",
        "    for k in range(5):\n",
        "        val_idx = folds[k]\n",
        "        train_idx = np.concatenate([folds[i] for i in range(5) if i != k])\n",
        "\n",
        "        X_train_b, y_train = Xb[train_idx], y[train_idx]\n",
        "        X_val_b,   y_val   = Xb[val_idx],   y[val_idx]\n",
        "\n",
        "        # Fit ridge model and predict\n",
        "        theta = ridge_fit(X_train_b, y_train, lam)\n",
        "        y_hat = X_val_b @ theta\n",
        "\n",
        "        # Compute sqrt(RSS) for this fold\n",
        "        s = sqrt_rss(y_val, y_hat)\n",
        "        sqrt_rss_per_fold.append(s)\n",
        "        print(f\"Fold {k+1}: sqrt_RSS = {s:.4f}\")\n",
        "\n",
        "    avg_sqrt_rss = np.mean(sqrt_rss_per_fold)\n",
        "    print(f\"Average sqrt_RSS over 5 folds (Œª={lam}) = {avg_sqrt_rss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "# --- TRAIN FINAL RIDGE MODEL USING ALL DATA (Œª = 1) ---\n",
        "X = features.astype(float)\n",
        "y = labels.astype(float).ravel()\n",
        "\n",
        "def add_bias(A):\n",
        "    return np.hstack([np.ones((A.shape[0], 1)), A])\n",
        "\n",
        "def ridge_fit(Xb, y, lam):\n",
        "    d = Xb.shape[1]\n",
        "    R = np.eye(d)\n",
        "    R[0, 0] = 0.0  # don't regularize bias\n",
        "    XtX = Xb.T @ Xb\n",
        "    Xty = Xb.T @ y\n",
        "    theta = np.linalg.pinv(XtX + lam * R) @ Xty\n",
        "    return theta\n",
        "\n",
        "# Add bias and fit model\n",
        "Xb = add_bias(X)\n",
        "lambda_best = 1\n",
        "theta_final = ridge_fit(Xb, y, lambda_best)\n",
        "\n",
        "print(\"Model trained successfully! Shape of weights:\", theta_final.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5xmae5PA48e",
        "outputId": "d42175e5-f972-41c1-a67c-bcba47c4ce7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained successfully! Shape of weights: (22,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "# --- FUNCTION TO PREDICT COMBAT POINT FOR A NEW POKEMON ---\n",
        "\n",
        "def predict_combat_point(stamina, attack, defense, capture_rate, flee_rate, spawn_chance, primary_strength):\n",
        "    # Numeric features (same order as training)\n",
        "    numeric_values = np.array([stamina, attack, defense, capture_rate, flee_rate, spawn_chance], dtype=float)\n",
        "\n",
        "    # Standardize numeric values using training means and stds\n",
        "    numeric_standardized = (numeric_values - mean) / std\n",
        "\n",
        "    # One-hot encode the primary_strength using the same categories\n",
        "    one_hot = np.zeros(len(unique_strengths))\n",
        "    if primary_strength in unique_strengths:\n",
        "        idx = np.where(unique_strengths == primary_strength)[0][0]\n",
        "        one_hot[idx] = 1.0\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Unknown strength '{primary_strength}'. Valid options are: {unique_strengths}\")\n",
        "\n",
        "    # Combine numeric + one-hot\n",
        "    features_new = np.hstack([numeric_standardized, one_hot])\n",
        "\n",
        "    # Add bias\n",
        "    features_new_b = np.hstack([1.0, features_new])\n",
        "\n",
        "    # Predict combat point\n",
        "    predicted_cp = features_new_b @ theta_final\n",
        "    return predicted_cp\n",
        "# Example 1: Fire-type Pok√©mon\n",
        "pred1 = predict_combat_point(120, 150, 100, 0.4, 0.1, 0.8, \"Fire\")\n",
        "print(f\"üî• Predicted Combat Point (Fire-type) = {pred1:.2f}\")\n",
        "\n",
        "# Example 2: Water-type Pok√©mon\n",
        "pred2 = predict_combat_point(130, 140, 110, 0.5, 0.2, 0.7, \"Water\")\n",
        "print(f\"üíß Predicted Combat Point (Water-type) = {pred2:.2f}\")\n",
        "\n",
        "# Example 3: Grass-type Pok√©mon\n",
        "pred3 = predict_combat_point(100, 120, 80, 0.6, 0.3, 0.9, \"Grass\")\n",
        "print(f\"üåø Predicted Combat Point (Grass-type) = {pred3:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ujT751dA_v9",
        "outputId": "6bf78d07-4571-404c-e134-1e180912c7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî• Predicted Combat Point (Fire-type) = 1304.83\n",
            "üíß Predicted Combat Point (Water-type) = 1353.23\n",
            "üåø Predicted Combat Point (Grass-type) = 846.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdZ8O_2uJ9uA"
      },
      "source": [
        "## Part 2: Neural Networks (40 points)\n",
        "\n",
        "In this part, you are going to implement your multi-layer perceptron model by the Pytorch library. You will still use the same handwritten digit image dataset from HW1. So, in the next few cells, please run the provided code to load and process the data, and create dataset objects for further use by Pytorch.\n",
        "\n",
        "**Note: you need to install Pytorch beforehand (check out https://pytorch.org/get-started/locally/). Or, you can use Google Colab for this homework, which is recommended.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXVq39QKJ9uA"
      },
      "outputs": [],
      "source": [
        "# load data from file and split into training and validation sets\n",
        "import numpy as np\n",
        "data = np.loadtxt(\"/content/drive/MyDrive/hw2/train.txt\", delimiter=',')\n",
        "perm_idx = np.random.permutation(data.shape[0])\n",
        "vali_num = int(data.shape[0] * 0.2)\n",
        "vali_idx = perm_idx[:vali_num]\n",
        "train_idx = perm_idx[vali_num:]\n",
        "train_data = data[train_idx]\n",
        "vali_data = data[vali_idx]\n",
        "train_features = train_data[:, 1:].astype(np.float32)\n",
        "train_labels = train_data[:, 0].astype(int)\n",
        "vali_features = vali_data[:, 1:].astype(np.float32)\n",
        "vali_labels = vali_data[:, 0].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6HuT_w7J9uA"
      },
      "outputs": [],
      "source": [
        "# define a Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx, :], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3wJWORRJ9uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8073e50-1a61-4876-ac96-225a78fecefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, F]: torch.Size([64, 784]) torch.float32\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "training_data = MNISTDataset(train_features, train_labels)\n",
        "vali_data = MNISTDataset(vali_features, vali_labels)\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "vali_dataloader = DataLoader(vali_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in train_dataloader:\n",
        "    print(f\"Shape of X [N, F]: {X.shape} {X.dtype}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvY98GZJ9uB"
      },
      "source": [
        "Now, you should have the train_dataloader and vali_dataloader. Then, you need to build and train your multi-layer perceptron model by Pytorch.\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html gives a comprehansive example how to achieve this. Please read this tutorial closely, and implement the model in the next few cells.\n",
        "\n",
        "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c30c1dcf2bc20119bcda7e734ce0eb42/quickstart_tutorial.ipynb provides the interactive version, which you can run and edit.\n",
        "\n",
        "**Note: in your implementation:**\n",
        "- you will only have three layers [784 -> 512 -> 10], you need to remove the [512 -> 512] layer in the tutorial.\n",
        "- add 'weight_decay=1e-4' in torch.optim.SGD to add L2 regularization.\n",
        "- train the model for 10 epochs instead of 5 epochs.\n",
        "- keep all other hyper-parameters the same as used in the tutorial.\n",
        "- **You are allowed to resue the code in the tutorial for this homework**\n",
        "\n",
        "\n",
        "**Note: print out the training process and the final accuracy on the validation set.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free (open a colab notebook, then Runtime->Change runtime type->Hardware accelerator->GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj1UdTgsJ9uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dad5845-7bab-42be-8f06-5e2f4cc380c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "MLP(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 01 | Batch  100/750 | Loss: 3.9878\n",
            "Epoch 01 | Batch  200/750 | Loss: 2.3422\n",
            "Epoch 01 | Batch  300/750 | Loss: 1.7108\n",
            "Epoch 01 | Batch  400/750 | Loss: 1.3825\n",
            "Epoch 01 | Batch  500/750 | Loss: 1.1706\n",
            "Epoch 01 | Batch  600/750 | Loss: 1.0287\n",
            "Epoch 01 | Batch  700/750 | Loss: 0.9215\n",
            "Epoch 01 | Batch  750/750 | Loss: 0.8764\n",
            "Epoch 01 complete. Validation Loss: 0.2686 | Validation Accuracy: 93.72%\n",
            "Epoch 02 | Batch  100/750 | Loss: 0.1742\n",
            "Epoch 02 | Batch  200/750 | Loss: 0.1739\n",
            "Epoch 02 | Batch  300/750 | Loss: 0.1593\n",
            "Epoch 02 | Batch  400/750 | Loss: 0.1562\n",
            "Epoch 02 | Batch  500/750 | Loss: 0.1523\n",
            "Epoch 02 | Batch  600/750 | Loss: 0.1506\n",
            "Epoch 02 | Batch  700/750 | Loss: 0.1473\n",
            "Epoch 02 | Batch  750/750 | Loss: 0.1456\n",
            "Epoch 02 complete. Validation Loss: 0.2192 | Validation Accuracy: 94.70%\n",
            "Epoch 03 | Batch  100/750 | Loss: 0.0935\n",
            "Epoch 03 | Batch  200/750 | Loss: 0.0915\n",
            "Epoch 03 | Batch  300/750 | Loss: 0.0832\n",
            "Epoch 03 | Batch  400/750 | Loss: 0.0836\n",
            "Epoch 03 | Batch  500/750 | Loss: 0.0821\n",
            "Epoch 03 | Batch  600/750 | Loss: 0.0823\n",
            "Epoch 03 | Batch  700/750 | Loss: 0.0807\n",
            "Epoch 03 | Batch  750/750 | Loss: 0.0803\n",
            "Epoch 03 complete. Validation Loss: 0.2023 | Validation Accuracy: 95.00%\n",
            "Epoch 04 | Batch  100/750 | Loss: 0.0587\n",
            "Epoch 04 | Batch  200/750 | Loss: 0.0568\n",
            "Epoch 04 | Batch  300/750 | Loss: 0.0512\n",
            "Epoch 04 | Batch  400/750 | Loss: 0.0519\n",
            "Epoch 04 | Batch  500/750 | Loss: 0.0508\n",
            "Epoch 04 | Batch  600/750 | Loss: 0.0514\n",
            "Epoch 04 | Batch  700/750 | Loss: 0.0502\n",
            "Epoch 04 | Batch  750/750 | Loss: 0.0502\n",
            "Epoch 04 complete. Validation Loss: 0.1930 | Validation Accuracy: 95.25%\n",
            "Epoch 05 | Batch  100/750 | Loss: 0.0392\n",
            "Epoch 05 | Batch  200/750 | Loss: 0.0378\n",
            "Epoch 05 | Batch  300/750 | Loss: 0.0339\n",
            "Epoch 05 | Batch  400/750 | Loss: 0.0344\n",
            "Epoch 05 | Batch  500/750 | Loss: 0.0338\n",
            "Epoch 05 | Batch  600/750 | Loss: 0.0341\n",
            "Epoch 05 | Batch  700/750 | Loss: 0.0334\n",
            "Epoch 05 | Batch  750/750 | Loss: 0.0333\n",
            "Epoch 05 complete. Validation Loss: 0.1878 | Validation Accuracy: 95.43%\n",
            "Epoch 06 | Batch  100/750 | Loss: 0.0272\n",
            "Epoch 06 | Batch  200/750 | Loss: 0.0264\n",
            "Epoch 06 | Batch  300/750 | Loss: 0.0235\n",
            "Epoch 06 | Batch  400/750 | Loss: 0.0239\n",
            "Epoch 06 | Batch  500/750 | Loss: 0.0235\n",
            "Epoch 06 | Batch  600/750 | Loss: 0.0236\n",
            "Epoch 06 | Batch  700/750 | Loss: 0.0231\n",
            "Epoch 06 | Batch  750/750 | Loss: 0.0231\n",
            "Epoch 06 complete. Validation Loss: 0.1855 | Validation Accuracy: 95.58%\n",
            "Epoch 07 | Batch  100/750 | Loss: 0.0197\n",
            "Epoch 07 | Batch  200/750 | Loss: 0.0192\n",
            "Epoch 07 | Batch  300/750 | Loss: 0.0170\n",
            "Epoch 07 | Batch  400/750 | Loss: 0.0175\n",
            "Epoch 07 | Batch  500/750 | Loss: 0.0172\n",
            "Epoch 07 | Batch  600/750 | Loss: 0.0172\n",
            "Epoch 07 | Batch  700/750 | Loss: 0.0168\n",
            "Epoch 07 | Batch  750/750 | Loss: 0.0168\n",
            "Epoch 07 complete. Validation Loss: 0.1838 | Validation Accuracy: 95.63%\n",
            "Epoch 08 | Batch  100/750 | Loss: 0.0141\n",
            "Epoch 08 | Batch  200/750 | Loss: 0.0141\n",
            "Epoch 08 | Batch  300/750 | Loss: 0.0126\n",
            "Epoch 08 | Batch  400/750 | Loss: 0.0132\n",
            "Epoch 08 | Batch  500/750 | Loss: 0.0128\n",
            "Epoch 08 | Batch  600/750 | Loss: 0.0128\n",
            "Epoch 08 | Batch  700/750 | Loss: 0.0125\n",
            "Epoch 08 | Batch  750/750 | Loss: 0.0125\n",
            "Epoch 08 complete. Validation Loss: 0.1839 | Validation Accuracy: 95.65%\n",
            "Epoch 09 | Batch  100/750 | Loss: 0.0102\n",
            "Epoch 09 | Batch  200/750 | Loss: 0.0105\n",
            "Epoch 09 | Batch  300/750 | Loss: 0.0095\n",
            "Epoch 09 | Batch  400/750 | Loss: 0.0101\n",
            "Epoch 09 | Batch  500/750 | Loss: 0.0099\n",
            "Epoch 09 | Batch  600/750 | Loss: 0.0098\n",
            "Epoch 09 | Batch  700/750 | Loss: 0.0096\n",
            "Epoch 09 | Batch  750/750 | Loss: 0.0096\n",
            "Epoch 09 complete. Validation Loss: 0.1839 | Validation Accuracy: 95.75%\n",
            "Epoch 10 | Batch  100/750 | Loss: 0.0075\n",
            "Epoch 10 | Batch  200/750 | Loss: 0.0080\n",
            "Epoch 10 | Batch  300/750 | Loss: 0.0074\n",
            "Epoch 10 | Batch  400/750 | Loss: 0.0080\n",
            "Epoch 10 | Batch  500/750 | Loss: 0.0077\n",
            "Epoch 10 | Batch  600/750 | Loss: 0.0077\n",
            "Epoch 10 | Batch  700/750 | Loss: 0.0076\n",
            "Epoch 10 | Batch  750/750 | Loss: 0.0076\n",
            "Epoch 10 complete. Validation Loss: 0.1842 | Validation Accuracy: 95.83%\n",
            "\n",
            "‚úÖ Final Validation Accuracy: 95.83%\n"
          ]
        }
      ],
      "source": [
        "# Write your code\n",
        "\n",
        "# --- PART 2: Multi-Layer Perceptron (784 -> 512 -> 10) using PyTorch ---\n",
        "# HW2 Data Mining :: CS 584 :: GMU\n",
        "# You must have train_dataloader and vali_dataloader ready before running this cell.\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set random seed and device\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Define the MLP model\n",
        "# -------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Flatten ensures the input (64, 784) remains consistent\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Only three layers: [784 -> 512 -> 10]\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate model\n",
        "model = MLP().to(device)\n",
        "print(model)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Define Loss Function & Optimizer\n",
        "# -------------------------\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-3\n",
        "# add weight_decay = 1e-4 for L2 regularization\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Training and Evaluation Loops\n",
        "# -------------------------\n",
        "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    running_loss = 0.0\n",
        "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch % 100 == 0 or batch == len(dataloader):\n",
        "            avg_loss = running_loss / batch\n",
        "            print(f\"Epoch {epoch:02d} | Batch {batch:4d}/{len(dataloader)} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def evaluate(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    num_batches = len(dataloader)\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            total_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            total += y.size(0)\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Train for 10 epochs\n",
        "# -------------------------\n",
        "epochs = 10\n",
        "for t in range(1, epochs + 1):\n",
        "    train(train_dataloader, model, loss_fn, optimizer, t)\n",
        "    val_loss, val_acc = evaluate(vali_dataloader, model, loss_fn)\n",
        "    print(f\"Epoch {t:02d} complete. Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ Final Validation Accuracy:\", f\"{val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJC16_GvJ9uB"
      },
      "source": [
        "## Part 3: Tune Hyperparameter (20 points)\n",
        "\n",
        "In this part, you need to do your best to tune the hyperparameters in the MLP to build the best model and evaluate the predictions for the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FsG2edRJ9uB"
      },
      "source": [
        "Now, you should tune four hyperparameters:\n",
        "\n",
        "- the number of layers and the dimension of each layer (explore as much as you can, but choose reasonable settings considering the computational resources you have)\n",
        "- the activation function (choose from sigmoid, tanh, relu)\n",
        "- weight decay\n",
        "- number of training epochs\n",
        "\n",
        "\n",
        "**Hint: You can tune these hyperparameters by one randomly generated validation set, or you can also use the cross-validation method.**\n",
        "\n",
        "**Note: you can use Colab for running the code with GPU for free**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WNqwRRl0J9uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bd9182-8116-426b-d480-78dc09624b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Total runs: 180\n",
            "[1/180] arch=[256], act=relu, wd=0.0, ep=5 => ValAcc=95.26% (loss=0.1794)\n",
            "[2/180] arch=[256], act=relu, wd=0.0, ep=10 => ValAcc=96.17% (loss=0.1515)\n",
            "[3/180] arch=[256], act=relu, wd=0.0, ep=15 => ValAcc=96.14% (loss=0.1683)\n",
            "[4/180] arch=[256], act=relu, wd=1e-05, ep=5 => ValAcc=95.06% (loss=0.1812)\n",
            "[5/180] arch=[256], act=relu, wd=1e-05, ep=10 => ValAcc=95.86% (loss=0.1672)\n",
            "[6/180] arch=[256], act=relu, wd=1e-05, ep=15 => ValAcc=96.00% (loss=0.1727)\n",
            "[7/180] arch=[256], act=relu, wd=0.0001, ep=5 => ValAcc=95.30% (loss=0.1802)\n",
            "[8/180] arch=[256], act=relu, wd=0.0001, ep=10 => ValAcc=96.04% (loss=0.1712)\n",
            "[9/180] arch=[256], act=relu, wd=0.0001, ep=15 => ValAcc=96.22% (loss=0.1711)\n",
            "[10/180] arch=[256], act=relu, wd=0.001, ep=5 => ValAcc=95.33% (loss=0.1768)\n",
            "[11/180] arch=[256], act=relu, wd=0.001, ep=10 => ValAcc=95.95% (loss=0.1674)\n",
            "[12/180] arch=[256], act=relu, wd=0.001, ep=15 => ValAcc=95.91% (loss=0.1660)\n",
            "[13/180] arch=[256], act=tanh, wd=0.0, ep=5 => ValAcc=90.97% (loss=0.3283)\n",
            "[14/180] arch=[256], act=tanh, wd=0.0, ep=10 => ValAcc=92.82% (loss=0.2621)\n",
            "[15/180] arch=[256], act=tanh, wd=0.0, ep=15 => ValAcc=93.54% (loss=0.2339)\n",
            "[16/180] arch=[256], act=tanh, wd=1e-05, ep=5 => ValAcc=91.11% (loss=0.3326)\n",
            "[17/180] arch=[256], act=tanh, wd=1e-05, ep=10 => ValAcc=92.55% (loss=0.2652)\n",
            "[18/180] arch=[256], act=tanh, wd=1e-05, ep=15 => ValAcc=93.38% (loss=0.2373)\n",
            "[19/180] arch=[256], act=tanh, wd=0.0001, ep=5 => ValAcc=90.93% (loss=0.3301)\n",
            "[20/180] arch=[256], act=tanh, wd=0.0001, ep=10 => ValAcc=92.73% (loss=0.2651)\n",
            "[21/180] arch=[256], act=tanh, wd=0.0001, ep=15 => ValAcc=93.23% (loss=0.2342)\n",
            "[22/180] arch=[256], act=tanh, wd=0.001, ep=5 => ValAcc=91.07% (loss=0.3346)\n",
            "[23/180] arch=[256], act=tanh, wd=0.001, ep=10 => ValAcc=92.26% (loss=0.2676)\n",
            "[24/180] arch=[256], act=tanh, wd=0.001, ep=15 => ValAcc=93.54% (loss=0.2311)\n",
            "[25/180] arch=[256], act=sigmoid, wd=0.0, ep=5 => ValAcc=88.78% (loss=0.5006)\n",
            "[26/180] arch=[256], act=sigmoid, wd=0.0, ep=10 => ValAcc=90.63% (loss=0.3649)\n",
            "[27/180] arch=[256], act=sigmoid, wd=0.0, ep=15 => ValAcc=91.57% (loss=0.3170)\n",
            "[28/180] arch=[256], act=sigmoid, wd=1e-05, ep=5 => ValAcc=88.56% (loss=0.5012)\n",
            "[29/180] arch=[256], act=sigmoid, wd=1e-05, ep=10 => ValAcc=90.84% (loss=0.3608)\n",
            "[30/180] arch=[256], act=sigmoid, wd=1e-05, ep=15 => ValAcc=91.62% (loss=0.3144)\n",
            "[31/180] arch=[256], act=sigmoid, wd=0.0001, ep=5 => ValAcc=88.81% (loss=0.4981)\n",
            "[32/180] arch=[256], act=sigmoid, wd=0.0001, ep=10 => ValAcc=90.52% (loss=0.3666)\n",
            "[33/180] arch=[256], act=sigmoid, wd=0.0001, ep=15 => ValAcc=91.81% (loss=0.3147)\n",
            "[34/180] arch=[256], act=sigmoid, wd=0.001, ep=5 => ValAcc=89.05% (loss=0.4962)\n",
            "[35/180] arch=[256], act=sigmoid, wd=0.001, ep=10 => ValAcc=90.55% (loss=0.3690)\n",
            "[36/180] arch=[256], act=sigmoid, wd=0.001, ep=15 => ValAcc=91.53% (loss=0.3172)\n",
            "[37/180] arch=[512], act=relu, wd=0.0, ep=5 => ValAcc=95.76% (loss=0.1731)\n",
            "[38/180] arch=[512], act=relu, wd=0.0, ep=10 => ValAcc=96.14% (loss=0.1637)\n",
            "[39/180] arch=[512], act=relu, wd=0.0, ep=15 => ValAcc=96.37% (loss=0.1647)\n",
            "[40/180] arch=[512], act=relu, wd=1e-05, ep=5 => ValAcc=95.53% (loss=0.1783)\n",
            "[41/180] arch=[512], act=relu, wd=1e-05, ep=10 => ValAcc=96.05% (loss=0.1695)\n",
            "[42/180] arch=[512], act=relu, wd=1e-05, ep=15 => ValAcc=96.42% (loss=0.1651)\n",
            "[43/180] arch=[512], act=relu, wd=0.0001, ep=5 => ValAcc=95.60% (loss=0.1754)\n",
            "[44/180] arch=[512], act=relu, wd=0.0001, ep=10 => ValAcc=96.18% (loss=0.1655)\n",
            "[45/180] arch=[512], act=relu, wd=0.0001, ep=15 => ValAcc=96.33% (loss=0.1666)\n",
            "[46/180] arch=[512], act=relu, wd=0.001, ep=5 => ValAcc=95.97% (loss=0.1715)\n",
            "[47/180] arch=[512], act=relu, wd=0.001, ep=10 => ValAcc=95.99% (loss=0.1679)\n",
            "[48/180] arch=[512], act=relu, wd=0.001, ep=15 => ValAcc=96.31% (loss=0.1656)\n",
            "[49/180] arch=[512], act=tanh, wd=0.0, ep=5 => ValAcc=91.47% (loss=0.3080)\n",
            "[50/180] arch=[512], act=tanh, wd=0.0, ep=10 => ValAcc=93.14% (loss=0.2472)\n",
            "[51/180] arch=[512], act=tanh, wd=0.0, ep=15 => ValAcc=93.43% (loss=0.2247)\n",
            "[52/180] arch=[512], act=tanh, wd=1e-05, ep=5 => ValAcc=91.52% (loss=0.3116)\n",
            "[53/180] arch=[512], act=tanh, wd=1e-05, ep=10 => ValAcc=93.07% (loss=0.2499)\n",
            "[54/180] arch=[512], act=tanh, wd=1e-05, ep=15 => ValAcc=93.35% (loss=0.2225)\n",
            "[55/180] arch=[512], act=tanh, wd=0.0001, ep=5 => ValAcc=91.53% (loss=0.3059)\n",
            "[56/180] arch=[512], act=tanh, wd=0.0001, ep=10 => ValAcc=92.80% (loss=0.2527)\n",
            "[57/180] arch=[512], act=tanh, wd=0.0001, ep=15 => ValAcc=93.71% (loss=0.2226)\n",
            "[58/180] arch=[512], act=tanh, wd=0.001, ep=5 => ValAcc=91.33% (loss=0.3112)\n",
            "[59/180] arch=[512], act=tanh, wd=0.001, ep=10 => ValAcc=92.73% (loss=0.2523)\n",
            "[60/180] arch=[512], act=tanh, wd=0.001, ep=15 => ValAcc=93.25% (loss=0.2300)\n",
            "[61/180] arch=[512], act=sigmoid, wd=0.0, ep=5 => ValAcc=89.43% (loss=0.4375)\n",
            "[62/180] arch=[512], act=sigmoid, wd=0.0, ep=10 => ValAcc=91.17% (loss=0.3332)\n",
            "[63/180] arch=[512], act=sigmoid, wd=0.0, ep=15 => ValAcc=91.92% (loss=0.2939)\n",
            "[64/180] arch=[512], act=sigmoid, wd=1e-05, ep=5 => ValAcc=89.58% (loss=0.4352)\n",
            "[65/180] arch=[512], act=sigmoid, wd=1e-05, ep=10 => ValAcc=91.08% (loss=0.3349)\n",
            "[66/180] arch=[512], act=sigmoid, wd=1e-05, ep=15 => ValAcc=91.84% (loss=0.2949)\n",
            "[67/180] arch=[512], act=sigmoid, wd=0.0001, ep=5 => ValAcc=89.42% (loss=0.4391)\n",
            "[68/180] arch=[512], act=sigmoid, wd=0.0001, ep=10 => ValAcc=91.04% (loss=0.3349)\n",
            "[69/180] arch=[512], act=sigmoid, wd=0.0001, ep=15 => ValAcc=91.77% (loss=0.2932)\n",
            "[70/180] arch=[512], act=sigmoid, wd=0.001, ep=5 => ValAcc=89.10% (loss=0.4445)\n",
            "[71/180] arch=[512], act=sigmoid, wd=0.001, ep=10 => ValAcc=90.99% (loss=0.3385)\n",
            "[72/180] arch=[512], act=sigmoid, wd=0.001, ep=15 => ValAcc=91.96% (loss=0.2953)\n",
            "[73/180] arch=[512, 256], act=relu, wd=0.0, ep=5 => ValAcc=95.58% (loss=0.1473)\n",
            "[74/180] arch=[512, 256], act=relu, wd=0.0, ep=10 => ValAcc=96.30% (loss=0.1301)\n",
            "[75/180] arch=[512, 256], act=relu, wd=0.0, ep=15 => ValAcc=96.91% (loss=0.1146)\n",
            "[76/180] arch=[512, 256], act=relu, wd=1e-05, ep=5 => ValAcc=95.73% (loss=0.1437)\n",
            "[77/180] arch=[512, 256], act=relu, wd=1e-05, ep=10 => ValAcc=96.66% (loss=0.1229)\n",
            "[78/180] arch=[512, 256], act=relu, wd=1e-05, ep=15 => ValAcc=96.64% (loss=0.1203)\n",
            "[79/180] arch=[512, 256], act=relu, wd=0.0001, ep=5 => ValAcc=95.97% (loss=0.1388)\n",
            "[80/180] arch=[512, 256], act=relu, wd=0.0001, ep=10 => ValAcc=96.58% (loss=0.1227)\n",
            "[81/180] arch=[512, 256], act=relu, wd=0.0001, ep=15 => ValAcc=96.70% (loss=0.1176)\n",
            "[82/180] arch=[512, 256], act=relu, wd=0.001, ep=5 => ValAcc=95.92% (loss=0.1344)\n",
            "[83/180] arch=[512, 256], act=relu, wd=0.001, ep=10 => ValAcc=96.73% (loss=0.1248)\n",
            "[84/180] arch=[512, 256], act=relu, wd=0.001, ep=15 => ValAcc=96.72% (loss=0.1244)\n",
            "[85/180] arch=[512, 256], act=tanh, wd=0.0, ep=5 => ValAcc=89.99% (loss=0.3773)\n",
            "[86/180] arch=[512, 256], act=tanh, wd=0.0, ep=10 => ValAcc=91.97% (loss=0.2901)\n",
            "[87/180] arch=[512, 256], act=tanh, wd=0.0, ep=15 => ValAcc=92.81% (loss=0.2516)\n",
            "[88/180] arch=[512, 256], act=tanh, wd=1e-05, ep=5 => ValAcc=90.02% (loss=0.3842)\n",
            "[89/180] arch=[512, 256], act=tanh, wd=1e-05, ep=10 => ValAcc=91.91% (loss=0.2881)\n",
            "[90/180] arch=[512, 256], act=tanh, wd=1e-05, ep=15 => ValAcc=92.78% (loss=0.2476)\n",
            "[91/180] arch=[512, 256], act=tanh, wd=0.0001, ep=5 => ValAcc=90.44% (loss=0.3738)\n",
            "[92/180] arch=[512, 256], act=tanh, wd=0.0001, ep=10 => ValAcc=91.87% (loss=0.2867)\n",
            "[93/180] arch=[512, 256], act=tanh, wd=0.0001, ep=15 => ValAcc=92.98% (loss=0.2514)\n",
            "[94/180] arch=[512, 256], act=tanh, wd=0.001, ep=5 => ValAcc=90.10% (loss=0.3818)\n",
            "[95/180] arch=[512, 256], act=tanh, wd=0.001, ep=10 => ValAcc=91.97% (loss=0.2866)\n",
            "[96/180] arch=[512, 256], act=tanh, wd=0.001, ep=15 => ValAcc=92.50% (loss=0.2540)\n",
            "[97/180] arch=[512, 256], act=sigmoid, wd=0.0, ep=5 => ValAcc=68.56% (loss=1.8994)\n",
            "[98/180] arch=[512, 256], act=sigmoid, wd=0.0, ep=10 => ValAcc=78.18% (loss=1.2860)\n",
            "[99/180] arch=[512, 256], act=sigmoid, wd=0.0, ep=15 => ValAcc=84.71% (loss=0.8242)\n",
            "[100/180] arch=[512, 256], act=sigmoid, wd=1e-05, ep=5 => ValAcc=69.40% (loss=1.8574)\n",
            "[101/180] arch=[512, 256], act=sigmoid, wd=1e-05, ep=10 => ValAcc=78.22% (loss=1.2478)\n",
            "[102/180] arch=[512, 256], act=sigmoid, wd=1e-05, ep=15 => ValAcc=84.15% (loss=0.8493)\n",
            "[103/180] arch=[512, 256], act=sigmoid, wd=0.0001, ep=5 => ValAcc=69.56% (loss=1.8819)\n",
            "[104/180] arch=[512, 256], act=sigmoid, wd=0.0001, ep=10 => ValAcc=77.38% (loss=1.2676)\n",
            "[105/180] arch=[512, 256], act=sigmoid, wd=0.0001, ep=15 => ValAcc=84.28% (loss=0.8509)\n",
            "[106/180] arch=[512, 256], act=sigmoid, wd=0.001, ep=5 => ValAcc=68.69% (loss=1.8860)\n",
            "[107/180] arch=[512, 256], act=sigmoid, wd=0.001, ep=10 => ValAcc=78.40% (loss=1.2732)\n",
            "[108/180] arch=[512, 256], act=sigmoid, wd=0.001, ep=15 => ValAcc=83.91% (loss=0.8530)\n",
            "[109/180] arch=[512, 512], act=relu, wd=0.0, ep=5 => ValAcc=96.14% (loss=0.1335)\n",
            "[110/180] arch=[512, 512], act=relu, wd=0.0, ep=10 => ValAcc=96.53% (loss=0.1270)\n",
            "[111/180] arch=[512, 512], act=relu, wd=0.0, ep=15 => ValAcc=96.57% (loss=0.1259)\n",
            "[112/180] arch=[512, 512], act=relu, wd=1e-05, ep=5 => ValAcc=96.17% (loss=0.1335)\n",
            "[113/180] arch=[512, 512], act=relu, wd=1e-05, ep=10 => ValAcc=96.51% (loss=0.1288)\n",
            "[114/180] arch=[512, 512], act=relu, wd=1e-05, ep=15 => ValAcc=96.67% (loss=0.1237)\n",
            "[115/180] arch=[512, 512], act=relu, wd=0.0001, ep=5 => ValAcc=96.16% (loss=0.1329)\n",
            "[116/180] arch=[512, 512], act=relu, wd=0.0001, ep=10 => ValAcc=96.58% (loss=0.1263)\n",
            "[117/180] arch=[512, 512], act=relu, wd=0.0001, ep=15 => ValAcc=96.92% (loss=0.1163)\n",
            "[118/180] arch=[512, 512], act=relu, wd=0.001, ep=5 => ValAcc=96.15% (loss=0.1335)\n",
            "[119/180] arch=[512, 512], act=relu, wd=0.001, ep=10 => ValAcc=96.62% (loss=0.1230)\n",
            "[120/180] arch=[512, 512], act=relu, wd=0.001, ep=15 => ValAcc=96.59% (loss=0.1244)\n",
            "[121/180] arch=[512, 512], act=tanh, wd=0.0, ep=5 => ValAcc=90.18% (loss=0.3654)\n",
            "[122/180] arch=[512, 512], act=tanh, wd=0.0, ep=10 => ValAcc=92.25% (loss=0.2783)\n",
            "[123/180] arch=[512, 512], act=tanh, wd=0.0, ep=15 => ValAcc=92.90% (loss=0.2396)\n",
            "[124/180] arch=[512, 512], act=tanh, wd=1e-05, ep=5 => ValAcc=90.23% (loss=0.3620)\n",
            "[125/180] arch=[512, 512], act=tanh, wd=1e-05, ep=10 => ValAcc=91.77% (loss=0.2814)\n",
            "[126/180] arch=[512, 512], act=tanh, wd=1e-05, ep=15 => ValAcc=92.77% (loss=0.2457)\n",
            "[127/180] arch=[512, 512], act=tanh, wd=0.0001, ep=5 => ValAcc=90.13% (loss=0.3632)\n",
            "[128/180] arch=[512, 512], act=tanh, wd=0.0001, ep=10 => ValAcc=92.10% (loss=0.2783)\n",
            "[129/180] arch=[512, 512], act=tanh, wd=0.0001, ep=15 => ValAcc=92.82% (loss=0.2435)\n",
            "[130/180] arch=[512, 512], act=tanh, wd=0.001, ep=5 => ValAcc=90.19% (loss=0.3663)\n",
            "[131/180] arch=[512, 512], act=tanh, wd=0.001, ep=10 => ValAcc=92.01% (loss=0.2792)\n",
            "[132/180] arch=[512, 512], act=tanh, wd=0.001, ep=15 => ValAcc=92.85% (loss=0.2426)\n",
            "[133/180] arch=[512, 512], act=sigmoid, wd=0.0, ep=5 => ValAcc=69.67% (loss=1.8299)\n",
            "[134/180] arch=[512, 512], act=sigmoid, wd=0.0, ep=10 => ValAcc=79.53% (loss=1.1402)\n",
            "[135/180] arch=[512, 512], act=sigmoid, wd=0.0, ep=15 => ValAcc=85.53% (loss=0.7510)\n",
            "[136/180] arch=[512, 512], act=sigmoid, wd=1e-05, ep=5 => ValAcc=71.47% (loss=1.8281)\n",
            "[137/180] arch=[512, 512], act=sigmoid, wd=1e-05, ep=10 => ValAcc=81.17% (loss=1.1077)\n",
            "[138/180] arch=[512, 512], act=sigmoid, wd=1e-05, ep=15 => ValAcc=84.58% (loss=0.7679)\n",
            "[139/180] arch=[512, 512], act=sigmoid, wd=0.0001, ep=5 => ValAcc=70.09% (loss=1.7755)\n",
            "[140/180] arch=[512, 512], act=sigmoid, wd=0.0001, ep=10 => ValAcc=80.02% (loss=1.1419)\n",
            "[141/180] arch=[512, 512], act=sigmoid, wd=0.0001, ep=15 => ValAcc=85.46% (loss=0.7474)\n",
            "[142/180] arch=[512, 512], act=sigmoid, wd=0.001, ep=5 => ValAcc=69.29% (loss=1.8150)\n",
            "[143/180] arch=[512, 512], act=sigmoid, wd=0.001, ep=10 => ValAcc=78.18% (loss=1.1702)\n",
            "[144/180] arch=[512, 512], act=sigmoid, wd=0.001, ep=15 => ValAcc=85.67% (loss=0.7550)\n",
            "[145/180] arch=[256, 256], act=relu, wd=0.0, ep=5 => ValAcc=95.67% (loss=0.1496)\n",
            "[146/180] arch=[256, 256], act=relu, wd=0.0, ep=10 => ValAcc=96.33% (loss=0.1282)\n",
            "[147/180] arch=[256, 256], act=relu, wd=0.0, ep=15 => ValAcc=96.51% (loss=0.1303)\n",
            "[148/180] arch=[256, 256], act=relu, wd=1e-05, ep=5 => ValAcc=95.79% (loss=0.1487)\n",
            "[149/180] arch=[256, 256], act=relu, wd=1e-05, ep=10 => ValAcc=96.46% (loss=0.1318)\n",
            "[150/180] arch=[256, 256], act=relu, wd=1e-05, ep=15 => ValAcc=96.58% (loss=0.1313)\n",
            "[151/180] arch=[256, 256], act=relu, wd=0.0001, ep=5 => ValAcc=95.81% (loss=0.1463)\n",
            "[152/180] arch=[256, 256], act=relu, wd=0.0001, ep=10 => ValAcc=96.28% (loss=0.1328)\n",
            "[153/180] arch=[256, 256], act=relu, wd=0.0001, ep=15 => ValAcc=96.69% (loss=0.1219)\n",
            "[154/180] arch=[256, 256], act=relu, wd=0.001, ep=5 => ValAcc=95.55% (loss=0.1572)\n",
            "[155/180] arch=[256, 256], act=relu, wd=0.001, ep=10 => ValAcc=96.04% (loss=0.1396)\n",
            "[156/180] arch=[256, 256], act=relu, wd=0.001, ep=15 => ValAcc=96.64% (loss=0.1293)\n",
            "[157/180] arch=[256, 256], act=tanh, wd=0.0, ep=5 => ValAcc=90.12% (loss=0.3908)\n",
            "[158/180] arch=[256, 256], act=tanh, wd=0.0, ep=10 => ValAcc=91.83% (loss=0.2997)\n",
            "[159/180] arch=[256, 256], act=tanh, wd=0.0, ep=15 => ValAcc=92.93% (loss=0.2505)\n",
            "[160/180] arch=[256, 256], act=tanh, wd=1e-05, ep=5 => ValAcc=90.08% (loss=0.3925)\n",
            "[161/180] arch=[256, 256], act=tanh, wd=1e-05, ep=10 => ValAcc=91.84% (loss=0.2947)\n",
            "[162/180] arch=[256, 256], act=tanh, wd=1e-05, ep=15 => ValAcc=92.58% (loss=0.2593)\n",
            "[163/180] arch=[256, 256], act=tanh, wd=0.0001, ep=5 => ValAcc=89.69% (loss=0.4022)\n",
            "[164/180] arch=[256, 256], act=tanh, wd=0.0001, ep=10 => ValAcc=92.07% (loss=0.2900)\n",
            "[165/180] arch=[256, 256], act=tanh, wd=0.0001, ep=15 => ValAcc=92.79% (loss=0.2528)\n",
            "[166/180] arch=[256, 256], act=tanh, wd=0.001, ep=5 => ValAcc=89.62% (loss=0.4006)\n",
            "[167/180] arch=[256, 256], act=tanh, wd=0.001, ep=10 => ValAcc=91.70% (loss=0.2925)\n",
            "[168/180] arch=[256, 256], act=tanh, wd=0.001, ep=15 => ValAcc=92.93% (loss=0.2503)\n",
            "[169/180] arch=[256, 256], act=sigmoid, wd=0.0, ep=5 => ValAcc=64.08% (loss=1.9617)\n",
            "[170/180] arch=[256, 256], act=sigmoid, wd=0.0, ep=10 => ValAcc=76.00% (loss=1.4588)\n",
            "[171/180] arch=[256, 256], act=sigmoid, wd=0.0, ep=15 => ValAcc=82.03% (loss=1.0277)\n",
            "[172/180] arch=[256, 256], act=sigmoid, wd=1e-05, ep=5 => ValAcc=66.31% (loss=1.9863)\n",
            "[173/180] arch=[256, 256], act=sigmoid, wd=1e-05, ep=10 => ValAcc=72.81% (loss=1.4388)\n",
            "[174/180] arch=[256, 256], act=sigmoid, wd=1e-05, ep=15 => ValAcc=80.17% (loss=1.0548)\n",
            "[175/180] arch=[256, 256], act=sigmoid, wd=0.0001, ep=5 => ValAcc=67.80% (loss=1.9709)\n",
            "[176/180] arch=[256, 256], act=sigmoid, wd=0.0001, ep=10 => ValAcc=75.98% (loss=1.4653)\n",
            "[177/180] arch=[256, 256], act=sigmoid, wd=0.0001, ep=15 => ValAcc=82.43% (loss=1.0132)\n",
            "[178/180] arch=[256, 256], act=sigmoid, wd=0.001, ep=5 => ValAcc=66.12% (loss=1.9997)\n",
            "[179/180] arch=[256, 256], act=sigmoid, wd=0.001, ep=10 => ValAcc=75.42% (loss=1.4547)\n",
            "[180/180] arch=[256, 256], act=sigmoid, wd=0.001, ep=15 => ValAcc=82.08% (loss=1.0036)\n",
            "\n",
            "Search finished in 7293.1 sec\n",
            "\n",
            "=== Top 10 runs by validation accuracy ===\n",
            "   run        arch activation  weight_decay  epochs  val_loss   val_acc  \\\n",
            "0  117  [512, 512]       relu       0.00010      15  0.116307  0.969250   \n",
            "1   75  [512, 256]       relu       0.00000      15  0.114566  0.969083   \n",
            "2   83  [512, 256]       relu       0.00100      10  0.124768  0.967333   \n",
            "3   84  [512, 256]       relu       0.00100      15  0.124419  0.967167   \n",
            "4   81  [512, 256]       relu       0.00010      15  0.117646  0.967000   \n",
            "5  153  [256, 256]       relu       0.00010      15  0.121860  0.966917   \n",
            "6  114  [512, 512]       relu       0.00001      15  0.123741  0.966750   \n",
            "7   77  [512, 256]       relu       0.00001      10  0.122865  0.966583   \n",
            "8  156  [256, 256]       relu       0.00100      15  0.129324  0.966417   \n",
            "9   78  [512, 256]       relu       0.00001      15  0.120299  0.966417   \n",
            "\n",
            "   seconds  \n",
            "0    82.78  \n",
            "1    51.94  \n",
            "2    39.88  \n",
            "3    60.27  \n",
            "4    59.01  \n",
            "5    39.39  \n",
            "6    76.30  \n",
            "7    43.10  \n",
            "8    40.59  \n",
            "9    58.26  \n",
            "\n",
            "=== Best configuration ===\n",
            "{'arch': [512, 512], 'activation': 'relu', 'weight_decay': 0.0001, 'epochs': 15, 'val_acc': 96.92, 'val_loss': 0.1163}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FlexibleMLP(\n",
              "  (net): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Write your code here\n",
        "# --- PART 3: Hyperparameter Tuning for MLP (PyTorch) ---\n",
        "# Tries multiple architectures, activations, weight_decay values, and epoch counts.\n",
        "# Reports the best validation accuracy and a summary table of all runs.\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import itertools\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Repro + device\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---------- Search Space ----------\n",
        "# Hidden layer architectures: each list is the sequence of hidden sizes\n",
        "architectures = [\n",
        "    [256],\n",
        "    [512],\n",
        "    [512, 256],\n",
        "    [512, 512],\n",
        "    [256, 256],\n",
        "]\n",
        "\n",
        "activations = {\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"sigmoid\": nn.Sigmoid,\n",
        "}\n",
        "\n",
        "weight_decays = [0.0, 1e-5, 1e-4, 1e-3]\n",
        "epoch_choices = [5, 10, 15]  # try short, medium, longer training\n",
        "\n",
        "# Keep other tutorial hyperparams the same\n",
        "learning_rate = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "class FlexibleMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Builds an MLP: 784 -> (hidden layers) -> 10\n",
        "    Activation placed between hidden layers only.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_layers, activation_name=\"relu\"):\n",
        "        super().__init__()\n",
        "        act = activations[activation_name]  # class, not instance\n",
        "        layers = []\n",
        "        in_dim = 784\n",
        "        for h in hidden_layers:\n",
        "            layers.append(nn.Linear(in_dim, h))\n",
        "            layers.append(act())\n",
        "            in_dim = h\n",
        "        layers.append(nn.Linear(in_dim, 10))\n",
        "        self.net = nn.Sequential(nn.Flatten(), *layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_one_model(model, train_loader, val_loader, epochs, weight_decay):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    def evaluate(loader):\n",
        "        model.eval()\n",
        "        total, correct, total_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y).item()\n",
        "                total_loss += loss\n",
        "                preds = logits.argmax(1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        return total_loss / max(1, len(loader)), correct / max(1, total)\n",
        "\n",
        "    # Train\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Final validation after last epoch\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "    return val_loss, val_acc, model\n",
        "\n",
        "# ---------- Grid/Random Search (here: grid over all combinations) ----------\n",
        "results = []\n",
        "best = {\"val_acc\": -1.0}\n",
        "\n",
        "combo_iter = list(itertools.product(architectures, activations.keys(), weight_decays, epoch_choices))\n",
        "print(f\"Total runs: {len(combo_iter)}\")\n",
        "\n",
        "start_all = time.time()\n",
        "for i, (arch, act, wd, ep) in enumerate(combo_iter, 1):\n",
        "    start = time.time()\n",
        "    model = FlexibleMLP(arch, act)\n",
        "    val_loss, val_acc, trained_model = train_one_model(model, train_dataloader, vali_dataloader, ep, wd)\n",
        "    dur = time.time() - start\n",
        "\n",
        "    results.append({\n",
        "        \"run\": i,\n",
        "        \"arch\": str(arch),\n",
        "        \"activation\": act,\n",
        "        \"weight_decay\": wd,\n",
        "        \"epochs\": ep,\n",
        "        \"val_loss\": float(val_loss),\n",
        "        \"val_acc\": float(val_acc),\n",
        "        \"seconds\": round(dur, 2),\n",
        "    })\n",
        "\n",
        "    if val_acc > best[\"val_acc\"]:\n",
        "        best.update({\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"arch\": arch,\n",
        "            \"activation\": act,\n",
        "            \"weight_decay\": wd,\n",
        "            \"epochs\": ep,\n",
        "            \"model_state\": trained_model.state_dict(),  # save best weights\n",
        "        })\n",
        "\n",
        "    print(f\"[{i}/{len(combo_iter)}] arch={arch}, act={act}, wd={wd}, ep={ep} \"\n",
        "          f\"=> ValAcc={val_acc*100:.2f}% (loss={val_loss:.4f})\")\n",
        "\n",
        "total_time = time.time() - start_all\n",
        "print(f\"\\nSearch finished in {total_time:.1f} sec\")\n",
        "\n",
        "summary_df = pd.DataFrame(results).sort_values(by=\"val_acc\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\n=== Top 10 runs by validation accuracy ===\")\n",
        "print(summary_df.head(10))\n",
        "\n",
        "print(\"\\n=== Best configuration ===\")\n",
        "print({\n",
        "    \"arch\": best[\"arch\"],\n",
        "    \"activation\": best[\"activation\"],\n",
        "    \"weight_decay\": best[\"weight_decay\"],\n",
        "    \"epochs\": best[\"epochs\"],\n",
        "    \"val_acc\": round(best[\"val_acc\"] * 100, 2),\n",
        "    \"val_loss\": round(best[\"val_loss\"], 4),\n",
        "})\n",
        "\n",
        "# (Optional) Rebuild and load best model for later test-time evaluation:\n",
        "best_model = FlexibleMLP(best[\"arch\"], best[\"activation\"]).to(device)\n",
        "best_model.load_state_dict(best[\"model_state\"])\n",
        "best_model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6EbGpXqJ9uB"
      },
      "source": [
        "### Question: What is your final hyperparameter setting? How do you tune them? What choices have you tried?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmeLT975J9uB"
      },
      "source": [
        " To find the best model, I experimented with different hyperparameters ‚Äî including the number of layers, activation functions, weight decay values, and training epochs.\n",
        "I tested architectures like [256], [512], [512, 256], [512, 512], and [256, 256] using activations (ReLU, tanh, sigmoid), weight decay (0, 1e-5, 1e-4, 1e-3), and epochs (5, 10, 15).\n",
        "After evaluating 180 runs, the best configuration was [512, 512] layers with ReLU, weight_decay = 0.0001, and 15 epochs, achieving about 96.9 % validation accuracy.\n",
        "ReLU consistently performed the best because it avoids vanishing gradients, and a small weight decay helped reduce overfitting for stable training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "CJKD8ymKJ9uB"
      },
      "source": [
        "Please apply the model with the best hyperparameter setting you find above to the testing set. First of all, let's load the testing data by executing the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sqSaCR28J9uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a56c58-4a9b-4a4e-9325-ed270ade8401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array of testing feature matrix: shape (10000, 784)\n",
            "array of testing labels: shape (10000,)\n"
          ]
        }
      ],
      "source": [
        "test_features = np.loadtxt(\"/content/drive/MyDrive/hw2/test.txt\", delimiter=',')\n",
        "test_labels = np.loadtxt(\"/content/drive/MyDrive/hw2/test_label.txt\")\n",
        "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
        "print('array of testing labels: shape ' + str(np.shape(test_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_dzY7mNJ9uB"
      },
      "source": [
        "Now, report the Accuracy of your best model on the testing set.\n",
        "\n",
        "**Hint: use the following two lines of code to generate the label predictions for test data:**\n",
        "- raw_pred = model(torch.tensor(test_features).to(device).float())\n",
        "- pred = np.argmax(raw_pred.to('cpu').detach().numpy(), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oGmrIVaHJ9uC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5feebaf8-9101-46f3-eaa7-ed0e2d079edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using best model: arch=[512, 512], activation=relu\n",
            "\n",
            "‚úÖ Test Accuracy of best model: 96.90%\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Final test evaluation code\n",
        "\n",
        "# 1) Ensure test labels are integer\n",
        "test_labels = test_labels.astype(int)\n",
        "\n",
        "# 2) Load the best model\n",
        "try:\n",
        "    best_model.eval()\n",
        "    used_model = best_model\n",
        "    used_arch = best[\"arch\"]\n",
        "    used_act  = best[\"activation\"]\n",
        "except NameError:\n",
        "    try:\n",
        "        used_arch = best[\"arch\"]\n",
        "        used_act  = best[\"activation\"]\n",
        "        used_model = FlexibleMLP(used_arch, used_act).to(device)\n",
        "        used_model.load_state_dict(best[\"model_state\"])\n",
        "        used_model.eval()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Best model not found. Please re-run the tuning cell first.\") from e\n",
        "\n",
        "print(f\"\\nUsing best model: arch={used_arch}, activation={used_act}\")\n",
        "\n",
        "# 3) Forward pass on test set\n",
        "with torch.no_grad():\n",
        "    raw_pred = used_model(torch.tensor(test_features).to(device).float())\n",
        "    pred = np.argmax(raw_pred.to('cpu').detach().numpy(), axis=1)\n",
        "\n",
        "# 4) Compute accuracy\n",
        "test_accuracy = (pred == test_labels).mean() * 100.0\n",
        "print(f\"\\n‚úÖ Test Accuracy of best model: {test_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2AInDFwtEBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}